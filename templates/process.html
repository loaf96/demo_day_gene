{% extends "layout.html" %}

{% block content %}

<div class="container">
    <div class="jumbotron mt-2">
    <h1 class="display-5 text-md-center">Process</h1>

    </div>
    <div class="row">
        <div class="col-md-2">
        </div>
        <div class="col-md-8">
        <div class="card-body">
            <p class="text-md-justified">
                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; To begin with, we needed to acquire a corpus of text that we could use to build our models off of.  We knew we would be dealing with a large set of potential texts, so we decided to use MongoDB as our database application to create a space to save all of our text.  We created the database, then developed a python script that would pull in and save the text we wanted to use.  We decided to initially use song lyrics as our input data.
                <br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; To do this, we wrote code using an API known as “lyricsgenius”.  This API connects us to the website LyricsGenius.com, and pulls a particular song’s lyrics from the website as a string.  We wrote a series of loops that would pull the top 25 songs from the imputed artist, and then save the text, along with the song name, and the artist name, inside of our database.  After we had successfully done this with song lyrics, we then used the website “Project Gutenberg” to acquire texts of books from famous and classic authors to subsequent collections that we could use to train our model on different styles of writing.
                <br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; At this point, we needed to clean our data, and convert it into a usable format to be fed into a neural network.  We used a series of scripts that tokenized the text, and further converted each of the words of text into a numeric value, while maintaining the order of the words.  We then split the words into input and target values. The input values act are used for the model to analyze before making a prediction and then the target value is the next value that our model is trying to predict. the amount of values to be taken into consideration is determined when one sets the sequence length. we then batch the dataset into groupings of the sequence length plus one. This is because the sequence length is the number of values to be taken into consideration that precede the value the model is trying to predict; making the addition of another element necessary for the model. otherwise it would not have anything to predict.
                <br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Now, we were ready to feed our data into a neural network model.  We used a basic structure at first, using an embedding layer, followed by a hidden layer, and then using a final dense layer.  After this was finished training, we saved the model weights to a separate file.  We then loaded the weights back into a function that read a seed text string of input, and used the model to predict what the next lines of text should be.  This then output a string of text generated by our model

                <br><br>
                <h3 class="text-center">Fine Tuning</h3>
                <br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This part of the process was the most time consuming and intensive.  We initially ran the code on the CPU of our computers, which we found to be far too slow.  We learned the methods required to run the Tensorflow on the GPU of our system, which proved to be 5-10X faster.
                We used a number of hyperparameters in our model, which allowed us to experiment and attempt to find the optimal setup.  These were our hyperparameters:
                <br><br>
                <ul>
                    <li>“Sequence length” which is how many words at a time the network considers while evaluating what word to predict next.</li>
                    <li>“Batch size” which is how many samples will be taken into consideration before the model makes any adjustments</li>
                    <li>“Buffer Size” the amount of samples that will be loaded into memory during training. If the buffer size is smaller than the dataset then it will cycle through to ensure all have been taken into consideration. </li>
                    <li>“Embedding dimensions” which is the number of relationships one wants to give each word, each of which receive a calculated value found by using one of various algorithms</li>
                    <li>“RNN Units” which is how many recursive neural net units are in each layer of the network</li>
                    In addition to the hyperparameters, we also had to decide on the number of layers included in the network, the types of layers, and the number of epochs to 
                </ul>
                    <br><br><h3 class="text-center">Technologies Used</h3>
                    <br><br>
                    <ul>
                        <li>Python</li>
                        <li>JavaScript</li>
                        <li>HTML</li>
                        <li>CSS</li>
                        <li>Bootstrap</li>
                        <li>Pandas</li>
                        <li>Plotly</li>
                        <li>Tensorflow</li>
                        <li>Keras</li>
                        <li>Numpy</li>
                        <li>LyricsGenius API</li>
                        <li>Jupyter Lab</li>
                        <li>Flask</li>
                    </ul>


                </ul>
            </p>
</div>

{% endblock content %}